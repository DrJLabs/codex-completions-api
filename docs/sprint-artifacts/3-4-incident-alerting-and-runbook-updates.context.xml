<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>3</epicId>
    <storyId>3.4</storyId>
    <title>Incident alerting and runbook updates</title>
    <status>ready-for-dev</status>
    <generatedAt>2025-11-21T00:12:31Z</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/stories/3-4-incident-alerting-and-runbook-updates.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>on-call engineer</asA>
    <iWant>actionable alerts and updated runbooks for the app-server path</iWant>
    <soThat>incidents are triaged quickly using the new signals</soThat>
    <tasks>- AC1: Author alert rules (latency/SLO, restart, error rate, tool_buffer anomalies) with thresholds/owners/paging and bounded labels; store under docs/app-server-migration/alerts/ and link to metrics schema.
- AC1: Implement alerts/dashboards in chosen stack; include validation script or check to ensure label hygiene (route/method/status_family/model) and no high-cardinality labels.
- AC2: Update runbooks with req_id trace stitching steps, log/metric query snippets, maintenance toggle flow, and escalation ladder; capture dry-run walkthrough (screens/logs) and store alongside the runbook.
- AC3: Publish dashboards with working links to trace-by-id helper and schema docs; attach screenshots or exported JSON; validate links in dev/prod stack.
- AC1-AC3 Testing: Trigger alert fire/drain in dev stack; record evidence for each AC; re-run lint/check to ensure references and link targets resolve.
- Tracking: Update sprint artifacts and changelog when status moves to ready-for-dev, and attach evidence paths in Dev Agent Record.</tasks>
  </story>

  <acceptanceCriteria>1) Alerts for latency/SLO breach, restart frequency, sustained error rate, and tool_buffer anomalies are defined with thresholds, owners, paging rules, and metric names/label sets constrained to {route,method,status_family,model}; thresholds include: P95 latency &gt; baseline+5% for 3m, restart count &gt;3 in 10m, HTTP 5xx ≥2% over 5m, tool_buffer_anomaly gauge &gt;0 for 2m.
2) Runbooks document req_id trace stitching (trace-by-id helper + log/metric queries), maintenance-mode interactions, and escalation flow; a dry-run exercise is recorded with steps and evidence links.
3) Incident dashboards link directly to the trace-by-id helper, schema/metric docs, and core panels (latency, error rate, restart/backoff, tool_buffer_*); all links are validated and screenshots/logs are stored as evidence.</acceptanceCriteria>

  <artifacts>
    <docs>
- path: docs/PRD.md | title: PRD | section: Functional Requirements | snippet: FR010 emits structured logs for worker lifecycle events and per-request summaries; FR011 publishes Prometheus-friendly metrics for throughput, latency, errors, restarts; FR012 persists short-lived JSON-RPC trace artifacts with PII scrubbing.
- path: docs/sprint-artifacts/tech-spec-epic-3.md | title: Epic 3 Tech Spec | section: Services and Modules / Data Models | snippet: Metrics follow Prometheus with labels {route, method, status_family, model}; tool_buffer_* gauges come from Epic 2; restart/backoff telemetry and maintenance flag align with supervisor backoff 250ms→5s and 10s drains.
- path: docs/architecture.md | title: Decision Architecture | section: Decision Summary | snippet: Observability uses structured JSON logs plus prom-client 15.1.3 for lifecycle, latency, restart counts; maintenance flag returns 503 with retry hints per operations decision; readiness/liveness stay the gating surface.
- path: docs/app-server-migration/metrics-and-alerts.md | title: App-Server Metrics and Alerts | section: Metric Surface / Label Hygiene | snippet: /metrics exposes codex_http_* latency/error buckets, codex_worker_restarts/backoff/ready, codex_tool_buffer_* and codex_maintenance_mode with guarded access and constrained labels (route, method, status_family, model; output_mode, reason).
- path: docs/app-server-migration/codex-completions-api-migration.md | title: Migration Runbook | section: Probe payload expectations | snippet: /readyz exposes restart/backoff metadata (restarts_total, next_restart_delay_ms, last_exit, startup_latency_ms, last_ready_at); readiness flips to 503 within &lt;5s on restart loops while /livez stays 200; guard alerts on backoff &gt;5s or restart growth.
    </docs>
    <code>
- path: src/services/metrics/index.js | kind: service | symbol: registry + codex_* metrics | lines: 1-180 | reason: Defines Prometheus counters/gauges/histograms for latency, errors, restarts, backoff, streams, tool_buffer events, maintenance, with label normalization for {route, method, status_family, model}.
- path: src/routes/health.js | kind: route | symbol: /healthz /readyz /livez | lines: 1-120 | reason: Readiness/liveness JSON surfaces supervisor restart/backoff metadata and backend_mode; align alerts/runbooks with these fields.
- path: scripts/dev-smoke.sh | kind: script | symbol: smoke flows | lines: n/a | reason: Dev/prod smoke hooks already scrape readiness and metrics; extend to validate alert fire/drain and label hygiene.
    </code>
    <dependencies>
- node: express@4.21.2
- prometheus: prom-client@15.1.3
- app-server: @openai/codex@0.58.0
- id generation: nanoid@5.1.6
    </dependencies>
  </artifacts>

  <constraints>- Reuse existing metrics/telemetry sources (src/services/metrics, supervisor snapshots); do not add high-cardinality labels beyond {route, method, status_family, model}.
- Alerts must consume restart/backoff data from supervisor/metrics; no duplicate collectors.
- Maintenance toggle must preserve 503 + Retry-After semantics and 10s graceful drains.
- Evidence (screens/logs) stored with dashboards/runbooks under docs/app-server-migration/.</constraints>
  <interfaces>- /metrics (guarded; Prometheus exposition)
- /healthz, /readyz, /livez (JSON with restart/backoff metadata)
- (guarded) maintenance toggle endpoint described in architecture/tech spec; returns 503 envelopes when enabled
- Trace-by-id helper referenced in runbooks (req_id stitching)</interfaces>
  <tests>
    <standards>Follow existing probe/metrics smoke patterns; use prom-client surface for alert fire/drain; evidence captured alongside runbooks.</standards>
    <locations>scripts/dev-smoke.sh; scripts/prod-smoke.sh; tests/integration (health/metrics)</locations>
    <ideas>- AC1: Trigger 5xx and latency spikes; confirm alerts fire and acknowledge drain; verify metric labels stay within allowed set.
- AC1: Force worker restarts/backoff; ensure restart alert fires with owner/page target.
- AC2: Run trace-by-id helper on req_id; verify runbook steps and escalation ladder; store dry-run evidence.
- AC3: Validate dashboard links to trace helper, schema docs, and panels (latency/error/restart/tool_buffer_*); capture screenshots JSON exports.</ideas>
  </tests>
</story-context>
