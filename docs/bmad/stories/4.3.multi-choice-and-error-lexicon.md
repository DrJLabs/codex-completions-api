---
title: Story 4.3 — Multi-Choice Support & Error Lexicon
status: Draft
version: 0.1
updated: 2025-09-24
epic: docs/bmad/stories/epic-chat-completions-canonical-parity.md
labels: [api, streaming, errors, multi_choice]
---

## Status

Draft — proceeding while Story 3.11 remains in Draft per `docs/bmad/issues/2025-09-24-story-3-11-follow-up.md`.

Depends on: Story 4.2 — Streaming Tool Call Blocks (Draft).

## Story

**As a** parity owner,  
**I want** multi-choice responses and error types to match OpenAI’s specifications,  
**so that** SDKs relying on multiple choices and canonical error handling work seamlessly with our proxy.

## Acceptance Criteria

1. `/v1/chat/completions` accepts `n>1` and returns multiple choices with deterministic index ordering in both non-stream and streaming modes; streaming deltas are grouped by `choices[index]`. [Sources: docs/bmad/research/2025-09-24-openai-chat-completions-streaming-reference.md#Field Notes; docs/openai-chat-completions-parity.md#Streaming Contract; https://help.openai.com/en/articles/8908601-controlling-generated-text-length]
2. Usage metrics honor `stream_options.include_usage:true` by including totals per choice (or aggregated as documented) without regressing single-choice behavior. [Source: docs/openai-chat-completions-parity.md#Streaming Contract]
3. Error envelopes use OpenAI’s canonical `type` values (`invalid_request_error`, `authentication_error`, `permission_error`, `not_found_error`, `tokens_exceeded_error`, `rate_limit_error`, `timeout_error`, `server_error`) and set `param` for validation failures (`model`, `messages`, `n`). [Sources: docs/openai-chat-completions-parity.md#Error Envelope (non‑2xx); https://help.openai.com/en/articles/6891829-api-error-codes]
4. Optional parameters (`logprobs`, `response_format`, `seed`, etc.) are accepted without functional changes, with errors only when a value is unsupported per OpenAI semantics. [Source: docs/openai-chat-completions-parity.md#Parameters (subset)]
5. Documentation and golden transcripts capture multi-choice streaming behavior and refreshed error-type matrix. [Source: docs/openai-chat-completions-parity.md#Streaming Contract]

## Tasks / Subtasks

- [ ] Update handlers (`src/handlers/chat/nonstream.js`, `src/handlers/chat/stream.js`) to generate multi-choice responses with per-index state and usage tracking. [Source: docs/bmad/architecture/source-tree.md#src/ Modules]
- [ ] Extend SSE pipeline to buffer deltas per choice and flush in index order (role → content/tool calls → finish → optional usage). [Sources: docs/bmad/research/2025-09-24-openai-chat-completions-streaming-reference.md#Lifecycle of a Stream; docs/bmad/architecture/sequence-stream.md]
- [ ] Refresh transcripts with a multi-choice scenario (e.g., `test-results/chat-completions/streaming-multi-choice.json`) and mark metadata with the research reference; capture deterministic seeds so solo dev can regenerate fixtures quickly. [Sources: docs/openai-chat-completions-parity.md#Golden Transcripts & Snapshots (Story 3.5); https://help.openai.com/en/articles/8908601-controlling-generated-text-length]
- [ ] Normalize error types in `src/lib/errors.js` (and callers) to the canonical set; add coverage for permission/not_found/timeouts. [Sources: docs/openai-chat-completions-parity.md#Error Envelope (non‑2xx); https://help.openai.com/en/articles/6891829-api-error-codes]
- [ ] Add integration tests for `n>1` happy path, validation errors, and updated error types; adjust Playwright/contract specs accordingly. [Sources: docs/openai-chat-completions-parity.md#Streaming Contract; tests/integration/chat.contract.streaming.int.test.js; https://help.openai.com/en/articles/8908601-controlling-generated-text-length]
- [ ] Document multi-choice guidance and error lexicon updates in `docs/openai-chat-completions-parity.md`. [Source: docs/openai-chat-completions-parity.md#Streaming Contract]

## Dev Notes

- Multi-choice streaming must preserve per-choice ordering; reuse `choices[index]` semantics observed in OpenAI responses. [Sources: docs/bmad/research/2025-09-24-openai-chat-completions-streaming-reference.md#Field Notes; https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models]
- Ensure usage accounting handles aggregated totals while optionally exposing per-choice counts if upstream provides them; coordinate with existing `emission_trigger` logic from Story 3.3. [Source: docs/openai-chat-completions-parity.md#Streaming Contract]
- Canonical error lexicon is already documented; centralize mapping in `src/lib/errors.js` so both handlers share the same builder. [Sources: docs/openai-chat-completions-parity.md#Error Envelope (non‑2xx); https://help.openai.com/en/articles/6891829-api-error-codes]
- Maintain compatibility toggles for stopping after tools (`PROXY_STOP_AFTER_TOOLS`) and concurrency guard logic when multiple choices stream simultaneously. [Source: docs/bmad/architecture/sequence-stream.md]
- Solo dev tip: script a quick CLI harness using `n=2` and differing prompts to capture both responses and verify deterministic transcript sanitizer output before pushing. [Source: https://help.openai.com/en/articles/8908601-controlling-generated-text-length]
- Consider telemetry updates to capture finish reasons per choice for debugging multi-choice flows. [Source: docs/bmad/architecture/source-tree.md#src/ Modules]

## Testing

- `npm run transcripts:generate -- --scenario streaming-multi-choice`. [Source: docs/openai-chat-completions-parity.md#Golden Transcripts & Snapshots (Story 3.5)]
- `npm run test:integration -- chat.contract.streaming.int.test.js tests/integration/chat.multi-choice.int.test.js`. [Source: docs/bmad/architecture/tech-stack.md#Testing & QA]
- `npm run verify:all` before merge to validate regression coverage. [Source: docs/bmad/architecture/tech-stack.md#Testing & QA]

## QA Results

Pending

## Change Log

| Date       | Version | Description                             | Author |
| ---------- | ------- | --------------------------------------- | ------ |
| 2025-09-24 | 0.1     | Initial draft for multi-choice & errors | sm     |
